{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Datasets Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, itertools\n",
    "from scipy.spatial.distance import euclidean\n",
    "import geopy.distance\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(X):\n",
    "    num_rows, num_cols = X.shape\n",
    "    M = np.empty((num_rows,num_rows))\n",
    "    M[:] = np.nan\n",
    "\n",
    "    for i, j in itertools.combinations(range(num_rows), 2):\n",
    "        d_ij = euclidean(X[i], X[j]) \n",
    "\n",
    "        sim = math.exp(-d_ij)\n",
    "        M[i][j] = sim\n",
    "        M[j][i] = sim\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        d_ii = euclidean(X[i], X[i]) \n",
    "        sim = math.exp(-d_ii)\n",
    "        M[i][i] = sim\n",
    "\n",
    "    return M\n",
    "\n",
    "def binarize_feature_matrix(df):\n",
    "\n",
    "    df = pd.concat([pd.get_dummies(df['state'], prefix='state'), df], axis=1)\n",
    "    df = pd.concat([pd.get_dummies(df['Noise Level'], prefix='noise'), df], axis=1)\n",
    "    df = pd.concat([pd.get_dummies(df['Attire'], prefix='attire'), df], axis=1)\n",
    "    df = pd.concat([pd.get_dummies(df['Alcohol'], prefix='alochol'), df], axis=1)\n",
    "    df = pd.concat([pd.get_dummies(df['Price_Range'], prefix='price'), df], axis=1)\n",
    "    df = pd.concat([pd.get_dummies(df['Wi_Fi'], prefix='price'), df], axis=1)\n",
    "\n",
    "    del_columns = [\"city\",\"state\",\"Noise Level\",\"Attire\",\"Alcohol\",\"Price_Range\",\"Wi_Fi\",\"business_id\",\"latitude\",\"longitude\"]\n",
    "    df_processed = df.drop(del_columns, axis=1)\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "def preprocess_feature_matrix(df):\n",
    "\n",
    "    df = df[df['review_count'] > 5] \n",
    "    del_columns = [\"full_address\",\"name\",\"review_count\",\"type\",\\\n",
    "                    \"Sunday_Open\",\"Sunday_Close\",\"Monday_Open\",\"Monday_Close\",\"Tuesday_Open\",\"Tuesday_Close\",\"Wednesday_Open\",\"Wednesday_Close\",\\\n",
    "                    \"Thursday_Open\",\"Thursday_Close\",\"Friday_Open\",\"Friday_Close\",\"Saturday_Open\",\"Saturday_Close\",\\\n",
    "                    \"Music_Background_Music\",\"Music_Jukebox\",\"Music_Live\",\"Music_Video\",\"Music_Karaoke\",\"Music_DJ\",\"Coat_Check\",\\\n",
    "                    \"Corkage\",\"BYOB\",\"Smoking\",\"Good_for_Dancing\",\"Happy_Hour\",\"Caters\",\"Drive-Thru\"]\n",
    "\n",
    "    df_processed = df.drop(del_columns, axis=1)\n",
    "    ht = {}\n",
    "    for col in df_processed:\n",
    "        try:   \n",
    "            ht[col] = (df_processed[col].isna().sum())/len(df_processed)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    df_processed_nona = df_processed.dropna()\n",
    "\n",
    "    #Min-Max Scaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    #Extract Las Vegas data\n",
    "    yelp_lasvegas_df = df_processed_nona[df_processed_nona['city']=='Las Vegas']\n",
    "    yelp_lasvegas_df.reset_index(drop=True, inplace=True)\n",
    "    print(\"Las Vegas dataset size:\", yelp_lasvegas_df.shape)\n",
    "    yelp_lasvegas_df_binarized = binarize_feature_matrix(yelp_lasvegas_df)\n",
    "    yelp_lasvegas_df_binarized_scaled = scaler.fit_transform(yelp_lasvegas_df_binarized)\n",
    "\n",
    "    #Extract Phoenix data\n",
    "    yelp_phoenix_df = df_processed_nona[df_processed_nona['city']=='Phoenix']\n",
    "    yelp_phoenix_df.reset_index(drop=True, inplace=True)\n",
    "    print(\"Phoenix dataset size:\", yelp_phoenix_df.shape)\n",
    "    yelp_phoenix_df_binarized = binarize_feature_matrix(yelp_phoenix_df)\n",
    "    yelp_phoenix_df_binarized_scaled = scaler.fit_transform(yelp_phoenix_df_binarized)\n",
    "\n",
    "    return yelp_lasvegas_df, yelp_lasvegas_df_binarized_scaled, yelp_phoenix_df, yelp_phoenix_df_binarized_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las Vegas dataset size: (3103, 57)\n",
      "Phoenix dataset size: (1849, 57)\n",
      "Computed feature matrices for Yelp data for Las Vegas and Phoenix.\n",
      "Computed similarity matrices for Yelp data for Las Vegas and Phoenix.\n",
      "Computed costs for Yelp data for Las Vegas and Phoenix.\n"
     ]
    }
   ],
   "source": [
    "#Read yelp rating data\n",
    "data_path = 'raw_data/yelp.csv'\n",
    "yelp_df = pd.read_csv(data_path)\n",
    "\n",
    "yelp_lasvegas_df, yelp_lasvegas_df_binarized_scaled, yelp_phoenix_df, yelp_phoenix_df_binarized_scaled = preprocess_feature_matrix(yelp_df)\n",
    "\n",
    "lasvegas_business_list = yelp_lasvegas_df[\"business_id\"].tolist()\n",
    "phoenix_business_list = yelp_phoenix_df[\"business_id\"].tolist()\n",
    "print(\"Computed feature matrices for Yelp data for Las Vegas and Phoenix.\")\n",
    "\n",
    "yelp_lasvegas_similarity_matrix = compute_similarity_matrix(yelp_lasvegas_df_binarized_scaled)\n",
    "yelp_phoenix_similarity_matrix = compute_similarity_matrix(yelp_phoenix_df_binarized_scaled)\n",
    "print(\"Computed similarity matrices for Yelp data for Las Vegas and Phoenix.\")\n",
    "\n",
    "#Compute costs based on distance from city center\n",
    "# city_coordinates  = {'Las Vegas':(36.166635, -115.147590), 'Phoenix':(33.451618, -112.074267), 'Charlotte':(35.225896, -80.843871), 'Pittsburgh':(40.441525, -79.999641), \\\n",
    "        #                     'Scottsdale':(33.495454, -111.925564)}\n",
    "\n",
    "yelp_phoenix_df['cost'] = yelp_phoenix_df.apply(lambda row: geopy.distance.distance((row['latitude'], row['longitude']), (33.451618, -112.074267)).km, axis=1)\n",
    "yelp_lasvegas_df['cost'] = yelp_lasvegas_df.apply(lambda row: geopy.distance.distance((row['latitude'], row['longitude']), (36.166635, -115.147590)).km, axis=1)\n",
    "\n",
    "yelp_phoenix_costs = yelp_phoenix_df['cost'].tolist()\n",
    "yelp_lasvegas_costs = yelp_lasvegas_df['cost'].tolist()\n",
    "print(\"Computed costs for Yelp data for Las Vegas and Phoenix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed graph distance matrices (km) for Yelp data for Las Vegas and Phoenix.\n"
     ]
    }
   ],
   "source": [
    "def compute_geo_distance_matrix(df):\n",
    "    coords = list(zip(df['latitude'].tolist(), df['longitude'].tolist()))\n",
    "    n = len(coords)\n",
    "    dist_mat = np.zeros((n, n), dtype=float)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            d_ij = geopy.distance.distance(coords[i], coords[j]).km\n",
    "            dist_mat[i, j] = d_ij\n",
    "            dist_mat[j, i] = d_ij\n",
    "    return dist_mat\n",
    "\n",
    "yelp_lasvegas_graphmat = compute_geo_distance_matrix(yelp_lasvegas_df)\n",
    "yelp_phoenix_graphmat = compute_geo_distance_matrix(yelp_phoenix_df)\n",
    "print(\"Computed graph distance matrices (km) for Yelp data for Las Vegas and Phoenix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Yelp data\n",
    "save_path = 'pickled_data/yelp/'\n",
    "\n",
    "with open(save_path + 'yelp_vegas_sim.pkl', 'wb') as f:\n",
    "    pickle.dump(yelp_lasvegas_similarity_matrix, f)\n",
    "\n",
    "with open(save_path + 'yelp_vegas_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(lasvegas_business_list, f)\n",
    "\n",
    "with open(save_path + 'yelp_vegas_costs.pkl', 'wb') as f:\n",
    "    pickle.dump(yelp_lasvegas_costs, f)\n",
    "\n",
    "with open(save_path + 'yelp_vegas_graphMat.pkl', 'wb') as f:\n",
    "    pickle.dump(yelp_lasvegas_graphmat, f)\n",
    "\n",
    "with open(save_path + 'yelp_phoenix_sim.pkl', 'wb') as f:\n",
    "    pickle.dump(yelp_phoenix_similarity_matrix, f)\n",
    "\n",
    "with open(save_path + 'yelp_phoneix_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(phoenix_business_list, f)\n",
    "\n",
    "with open(save_path + 'yelp_phoenix_costs.pkl', 'wb') as f:\n",
    "    pickle.dump(yelp_phoenix_costs, f)\n",
    "\n",
    "with open(save_path + 'yelp_phoenix_graphMat.pkl', 'wb') as f:\n",
    "    pickle.dump(yelp_phoenix_graphmat, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pareto_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
