{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team Formation Datasets Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create IMDB, Freelancer and Bbsm Datasets of different sizes\n",
    "import sys, os, pickle\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "from utils import *\n",
    "data_path = 'raw_data/teamformation/'\n",
    "\n",
    "#Cost values to assign to experts\n",
    "sampleCostVals = [i for i in range(5, 101, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freelancer-1 ## 135\n",
    "sizeUniv = 50\n",
    "experts_file = data_path + 'freelancer_experts.txt'\n",
    "tasks_file = data_path + 'freelancer_tasks.txt'\n",
    "freelancer_tasks, freelancer_experts, freelancer_expert_indices = importData(experts_file, tasks_file, numExperts=50)\n",
    "freelancer_tasks, freelancer_experts = reduceSkills(freelancer_tasks, freelancer_experts, num_skills=sizeUniv)\n",
    "#Cost array and edge weight matrix\n",
    "freelancer_expertCosts = [sampleCostVals[np.random.randint(low=0, high=len(sampleCostVals))] for i in range(len(freelancer_experts))]\n",
    "freelancer_edgeWeightMat = importGraphData(data_path + 'freelancer_graphMat.pkl', freelancer_expert_indices)\n",
    "\n",
    "\n",
    "## Freelancer-2 ## 557\n",
    "freelancer2_tasks, freelancer2_experts, freelancer2_expert_indices = importData(experts_file, tasks_file, numExperts=150)\n",
    "freelancer2_tasks, freelancer2_experts = reduceSkills(freelancer2_tasks, freelancer2_experts, num_skills=sizeUniv)\n",
    "#Cost array and edge weight matrix\n",
    "freelancer2_expertCosts = [sampleCostVals[np.random.randint(low=0, high=len(sampleCostVals))] for i in range(len(freelancer2_experts))]\n",
    "freelancer2_edgeWeightMat = importGraphData(data_path + 'freelancer_graphMat.pkl', freelancer2_expert_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Freelancer Summary statistics\n",
    "getDatasetSummary(freelancer_tasks[0:250], freelancer_experts, freelancer_edgeWeightMat, 'Freelancer-1')\n",
    "print(\"==\"*50)\n",
    "getDatasetSummary(freelancer2_tasks[0:250], freelancer2_experts, freelancer2_edgeWeightMat, 'Freelancer-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Freelancer-1 data\n",
    "save_path = 'pickled_data/freelancer/'\n",
    "\n",
    "with open(save_path + 'freelancer_tasks_1.pkl', 'wb') as f:\n",
    "    pickle.dump(freelancer_tasks[:250], f)\n",
    "\n",
    "with open(save_path + 'freelancer_experts_1.pkl', 'wb') as f:\n",
    "    pickle.dump(freelancer_experts, f)\n",
    "\n",
    "with open(save_path + 'freelancer_costs_1.pkl', 'wb') as f:\n",
    "    pickle.dump(freelancer_expertCosts, f)\n",
    "\n",
    "with open(save_path + 'freelancer_graphMat_1.pkl', 'wb') as f:\n",
    "    pickle.dump(freelancer_edgeWeightMat, f)\n",
    "\n",
    "##Save Freelancer-2 data\n",
    "with open(save_path + 'freelancer_tasks_2.pkl', 'wb') as f:\n",
    "    pickle.dump(freelancer2_tasks[:250], f)\n",
    "\n",
    "with open(save_path + 'freelancer_experts_2.pkl', 'wb') as f:\n",
    "    pickle.dump(freelancer2_experts, f)\n",
    "\n",
    "with open(save_path + 'freelancer_costs_2.pkl', 'wb') as f:\n",
    "    pickle.dump(freelancer2_expertCosts, f)\n",
    "\n",
    "with open(save_path + 'freelancer_graphMat_2.pkl', 'wb') as f:\n",
    "    pickle.dump(freelancer2_edgeWeightMat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## IMDB-1 ## 605\n",
    "experts_file = data_path + 'imdb_experts_1.txt'\n",
    "tasks_file = data_path + 'imdb_tasks_1.txt'\n",
    "imdb1_tasks, imdb1_experts, imdb1_expert_indices = importData(experts_file, tasks_file, numExperts=200)\n",
    "#Cost array and edge weight matrix\n",
    "imdb1_expertCosts = [sampleCostVals[np.random.randint(low=0, high=len(sampleCostVals))] for i in range(len(imdb1_experts))]\n",
    "imdb1_edgeWeightMat = importGraphData(data_path + 'imdb_graphMat_1.pkl', imdb1_expert_indices)\n",
    "    \n",
    "## IMDB-2 ## 1289\n",
    "experts_file = data_path + 'imdb_experts_2.txt'\n",
    "tasks_file = data_path + 'imdb_tasks_2.txt'\n",
    "imdb2_tasks, imdb2_experts, imdb2_expert_indices = importData(experts_file, tasks_file, numExperts=400)\n",
    "#Cost array and edge weight matrix\n",
    "imdb2_expertCosts = [sampleCostVals[np.random.randint(low=0, high=len(sampleCostVals))] for i in range(len(imdb2_experts))]\n",
    "imdb2_edgeWeightMat = importGraphData(data_path + 'imdb_graphMat_2.pkl', imdb2_expert_indices)\n",
    "\n",
    "##IMDB-3 ## 3460\n",
    "experts_file = data_path + 'imdb_experts_3.txt'\n",
    "tasks_file = data_path + 'imdb_tasks_3.txt'\n",
    "imdb3_tasks, imdb3_experts, imdb3_expert_indices = importData(experts_file, tasks_file, numExperts=1000)\n",
    "#Cost array and edge weight matrix\n",
    "imdb3_expertCosts = [sampleCostVals[np.random.randint(low=0, high=len(sampleCostVals))] for i in range(len(imdb3_experts))]\n",
    "imdb3_edgeWeightMat = importGraphData(data_path + 'imdb_graphMat_3.pkl', imdb3_expert_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDatasetSummary(imdb1_tasks[0:300], imdb1_experts, imdb1_edgeWeightMat, 'IMDB-1')\n",
    "print(\"==\"*50)\n",
    "getDatasetSummary(imdb2_tasks[0:300], imdb2_experts, imdb2_edgeWeightMat, 'IMDB-2')\n",
    "print(\"==\"*50)\n",
    "getDatasetSummary(imdb3_tasks[0:300], imdb3_experts, imdb3_edgeWeightMat, 'IMDB-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save IMDB-1 data\n",
    "save_path = 'pickled_data/imdb/'\n",
    "\n",
    "with open(save_path + 'imdb_tasks_1.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb1_tasks[:300], f)\n",
    "\n",
    "with open(save_path + 'imdb_experts_1.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb1_experts, f)\n",
    "\n",
    "with open(save_path + 'imdb_costs_1.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb1_expertCosts, f)\n",
    "\n",
    "with open(save_path + 'imdb_graphMat_1.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb1_edgeWeightMat, f)\n",
    "\n",
    "##Save IMDB-2 data\n",
    "with open(save_path + 'imdb_tasks_2.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb2_tasks[:300], f)\n",
    "\n",
    "with open(save_path + 'imdb_experts_2.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb2_experts, f)\n",
    "\n",
    "with open(save_path + 'imdb_costs_2.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb2_expertCosts, f)\n",
    "\n",
    "with open(save_path + 'imdb_graphMat_2.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb2_edgeWeightMat, f)\n",
    "\n",
    "##Save IMDB-3 data\n",
    "with open(save_path + 'imdb_tasks_3.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb3_tasks[:300], f)\n",
    "\n",
    "with open(save_path + 'imdb_experts_3.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb3_experts, f)\n",
    "\n",
    "with open(save_path + 'imdb_costs_3.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb3_expertCosts, f)\n",
    "\n",
    "with open(save_path + 'imdb_graphMat_3.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb3_edgeWeightMat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibsonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeUniv = 75\n",
    "## Bbsm-1 ## 316\n",
    "experts_file = data_path + 'bibs_experts_1.txt'\n",
    "tasks_file = data_path + 'bibs_tasks_1.txt'\n",
    "bibs1_tasks, bibs1_experts, bibs1_expert_indices = importData(experts_file, tasks_file, numExperts=250)\n",
    "bibs1_tasks, bibs1_experts = reduceSkills(bibs1_tasks, bibs1_experts, num_skills=sizeUniv)\n",
    "#Cost array and edge weight matrix\n",
    "bibs1_expertCosts = [sampleCostVals[np.random.randint(low=0, high=len(sampleCostVals))] for i in range(len(bibs1_experts))]\n",
    "bibs1_edgeWeightMat = importGraphData(data_path + 'bibs_graphMat_1.pkl', bibs1_expert_indices)\n",
    "\n",
    "## Bbsm-2## 610\n",
    "experts_file = data_path + 'bibs_experts_2.txt'\n",
    "tasks_file = data_path + 'bibs_tasks_2.txt'\n",
    "bibs2_tasks, bibs2_experts, bibs2_expert_indices = importData(experts_file, tasks_file, numExperts=500)\n",
    "bibs2_tasks, bibs2_experts = reduceSkills(bibs2_tasks, bibs2_experts, num_skills=sizeUniv)\n",
    "#Cost array and edge weight matrix\n",
    "bibs2_expertCosts = [sampleCostVals[np.random.randint(low=0, high=len(sampleCostVals))] for i in range(len(bibs2_experts))]\n",
    "bibs2_edgeWeightMat = importGraphData(data_path + 'bibs_graphMat_2.pkl', bibs2_expert_indices)\n",
    "\n",
    "##Bbsm-3## 1273\n",
    "bibs3_tasks, bibs3_experts, bibs3_expert_indices = importData(experts_file, tasks_file, numExperts=1000)\n",
    "bibs3_tasks, bibs3_experts = reduceSkills(bibs3_tasks, bibs3_experts, num_skills=sizeUniv)\n",
    "#Cost array and edge weight matrix\n",
    "bibs3_expertCosts = [sampleCostVals[np.random.randint(low=0, high=len(sampleCostVals))] for i in range(len(bibs3_experts))]\n",
    "bibs3_edgeWeightMat = importGraphData(data_path + 'bibs_graphMat_2.pkl', bibs3_expert_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDatasetSummary(bibs1_tasks[0:300], bibs1_experts, bibs1_edgeWeightMat, 'Bbsm-1')\n",
    "print(\"==\"*50)\n",
    "getDatasetSummary(bibs2_tasks[0:300], bibs2_experts, bibs2_edgeWeightMat, 'Bbsm-2')\n",
    "print(\"==\"*50)\n",
    "getDatasetSummary(bibs3_tasks[0:300], bibs3_experts, bibs3_edgeWeightMat, 'Bbsm-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Bbsm-1 data\n",
    "save_path = 'pickled_data/bbsm/'\n",
    "\n",
    "with open(save_path + 'bbsm_tasks_1.pkl', 'wb') as f:\n",
    "    pickle.dump(bibs1_tasks[:300], f)\n",
    "\n",
    "with open(save_path + 'bbsm_experts_1.pkl', 'wb') as f:\n",
    "    pickle.dump(bibs1_experts, f)\n",
    "\n",
    "with open(save_path + 'bbsm_costs_1.pkl', 'wb') as f:\n",
    "    pickle.dump(bibs1_expertCosts, f)\n",
    "\n",
    "with open(save_path + 'bbsm_graphMat_1.pkl', 'wb') as f:\n",
    "    pickle.dump(bibs1_edgeWeightMat, f)\n",
    "\n",
    "\n",
    "##Save Bbsm-2 data\n",
    "with open(save_path + 'bbsm_tasks_2.pkl', 'wb') as f:\n",
    "    pickle.dump(bibs2_tasks[:300], f)\n",
    "\n",
    "with open(save_path + 'bbsm_experts_2.pkl', 'wb') as f:\n",
    "    pickle.dump(bibs2_experts, f)\n",
    "\n",
    "with open(save_path + 'bbsm_costs_2.pkl', 'wb') as f:\n",
    "    pickle.dump(bibs2_expertCosts, f)\n",
    "\n",
    "with open(save_path + 'bbsm_graphMat_2.pkl', 'wb') as f:\n",
    "    pickle.dump(bibs2_edgeWeightMat, f)\n",
    "\n",
    "##Save Bbsm-3 data\n",
    "with open(save_path + 'bbsm_tasks_3.pkl', 'wb') as f:\n",
    "    pickle.dump(bibs3_tasks[:300], f)\n",
    "\n",
    "with open(save_path + 'bbsm_experts_3.pkl', 'wb') as f:\n",
    "    pickle.dump(bibs3_experts, f)\n",
    "\n",
    "with open(save_path + 'bbsm_costs_3.pkl', 'wb') as f:\n",
    "    pickle.dump(bibs3_expertCosts, f)\n",
    "\n",
    "with open(save_path + 'bbsm_graphMat_3.pkl', 'wb') as f:\n",
    "    pickle.dump(bibs3_edgeWeightMat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pareto_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
